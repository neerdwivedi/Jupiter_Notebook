{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN/vP4CXHgjAD3Z3N6FikDs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neerdwivedi/Jupiter_Notebook/blob/main/ML%20Project\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_N0z8MD_GsZ",
        "outputId": "50e9a454-07d4-490f-b00d-727166b81beb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn==1.3.0 in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.11/dist-packages (1.24.3)\n",
            "Requirement already satisfied: transformers==4.31.0 in /usr/local/lib/python3.11/dist-packages (4.31.0)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.11/dist-packages (2.0.1)\n",
            "Requirement already satisfied: streamlit==1.24.0 in /usr/local/lib/python3.11/dist-packages (1.24.0)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.0) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.0) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.24.0) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.24.0) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.24.0) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.24.0) (8.1.8)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.24.0) (6.11.0)\n",
            "Requirement already satisfied: pillow<10,>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.24.0) (9.5.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.24.0) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.24.0) (18.1.0)\n",
            "Requirement already satisfied: pympler<2,>=0.9 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.24.0) (1.1)\n",
            "Requirement already satisfied: rich<14,>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.24.0) (13.9.4)\n",
            "Requirement already satisfied: tenacity<9,>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.24.0) (8.5.0)\n",
            "Requirement already satisfied: toml<2 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.24.0) (0.10.2)\n",
            "Requirement already satisfied: tzlocal<5,>=1.1 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.24.0) (4.3.1)\n",
            "Requirement already satisfied: validators<1,>=0.2 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.24.0) (0.34.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.24.0) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.1.dev5 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.24.0) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.24.0) (6.4.2)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.11/dist-packages (from streamlit==1.24.0) (6.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (18.1.8)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit==1.24.0) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit==1.24.0) (1.35.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3->streamlit==1.24.0) (4.0.12)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<7,>=1.4->streamlit==1.24.0) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.11.0->streamlit==1.24.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.11.0->streamlit==1.24.0) (2.18.0)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.11/dist-packages (from tzlocal<5,>=1.1->streamlit==1.24.0) (0.1.0.post0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3->streamlit==1.24.0) (5.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.24.0) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.24.0) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.24.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.24.0) (0.24.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.11.0->streamlit==1.24.0) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn==1.3.0 pandas==2.0.3 numpy==1.24.3 transformers==4.31.0 torch==2.0.1 streamlit==1.24.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data/human_code data/ai_code\n"
      ],
      "metadata": {
        "id": "Y3bS582TAhvy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile code_classifier.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import os\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "class CodeClassifier:\n",
        "    def __init__(self):\n",
        "        self.traditional_model = None\n",
        "        self.vectorizer = None\n",
        "        self.transformer_model = None\n",
        "        self.tokenizer = None\n",
        "        self.supported_extensions = ['.py', '.js', '.java', '.cpp', '.c', '.html', '.css']\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "    def preprocess_code(self, code_content):\n",
        "        \"\"\"Preprocess code by removing comments, extra whitespace, etc.\"\"\"\n",
        "        # Remove comments\n",
        "        code_content = re.sub(r'#.*$', '', code_content, flags=re.MULTILINE)  # Python comments\n",
        "        code_content = re.sub(r'//.*$', '', code_content, flags=re.MULTILINE)  # Single line comments\n",
        "        code_content = re.sub(r'/\\*.*?\\*/', '', code_content, flags=re.DOTALL)  # Multi-line comments\n",
        "\n",
        "        # Remove empty lines and normalize whitespace\n",
        "        lines = [line.strip() for line in code_content.split('\\n')]\n",
        "        lines = [line for line in lines if line]\n",
        "        code_content = '\\n'.join(lines)\n",
        "\n",
        "        return code_content\n",
        "\n",
        "    def load_github_data(self, human_code_path='data/human_code'):\n",
        "        \"\"\"Load human-written code from local directory\"\"\"\n",
        "        human_code = []\n",
        "        file_count = 0\n",
        "        error_count = 0\n",
        "\n",
        "        # Get list of files first\n",
        "        all_files = []\n",
        "        for root, _, files in os.walk(human_code_path):\n",
        "            for file in files:\n",
        "                if any(file.endswith(ext) for ext in self.supported_extensions):\n",
        "                    all_files.append(os.path.join(root, file))\n",
        "\n",
        "        # Process files with progress bar\n",
        "        for file_path in tqdm(all_files, desc=\"Loading human code\"):\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    code_content = f.read()\n",
        "                    # Skip empty files or very short snippets\n",
        "                    if len(code_content.strip()) < 10:\n",
        "                        continue\n",
        "                    code_content = self.preprocess_code(code_content)\n",
        "                    human_code.append(code_content)\n",
        "                    file_count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading file {file_path}: {e}\")\n",
        "                error_count += 1\n",
        "                continue\n",
        "\n",
        "        print(f\"Loaded {len(human_code)} human code samples\")\n",
        "        print(f\"Processed {file_count} files with {error_count} errors\")\n",
        "        return human_code\n",
        "\n",
        "    def load_ai_generated_data(self, ai_code_path='data/ai_code'):\n",
        "        \"\"\"Load AI-generated code from local directory\"\"\"\n",
        "        ai_code = []\n",
        "        file_count = 0\n",
        "        error_count = 0\n",
        "\n",
        "        # Get list of files first\n",
        "        all_files = []\n",
        "        for root, _, files in os.walk(ai_code_path):\n",
        "            for file in files:\n",
        "                if any(file.endswith(ext) for ext in self.supported_extensions):\n",
        "                    all_files.append(os.path.join(root, file))\n",
        "\n",
        "        # Process files with progress bar\n",
        "        for file_path in tqdm(all_files, desc=\"Loading AI code\"):\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    code_content = f.read()\n",
        "                    # Skip empty files or very short snippets\n",
        "                    if len(code_content.strip()) < 10:\n",
        "                        continue\n",
        "                    code_content = self.preprocess_code(code_content)\n",
        "                    ai_code.append(code_content)\n",
        "                    file_count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading file {file_path}: {e}\")\n",
        "                error_count += 1\n",
        "                continue\n",
        "\n",
        "        print(f\"Loaded {len(ai_code)} AI code samples\")\n",
        "        print(f\"Processed {file_count} files with {error_count} errors\")\n",
        "        return ai_code\n",
        "\n",
        "    def prepare_data(self, human_code_path='data/human_code', ai_code_path='data/ai_code'):\n",
        "        \"\"\"Prepare and combine both human and AI code\"\"\"\n",
        "        human_code = self.load_github_data(human_code_path)\n",
        "        ai_code = self.load_ai_generated_data(ai_code_path)\n",
        "\n",
        "        if not human_code or not ai_code:\n",
        "            raise ValueError(\"No code samples found. Please check your data directories.\")\n",
        "\n",
        "        # Balance the dataset if needed\n",
        "        min_samples = min(len(human_code), len(ai_code))\n",
        "        if len(human_code) > min_samples:\n",
        "            human_code = human_code[:min_samples]\n",
        "        if len(ai_code) > min_samples:\n",
        "            ai_code = ai_code[:min_samples]\n",
        "\n",
        "        # Combine data\n",
        "        code_samples = human_code + ai_code\n",
        "        labels = [0] * len(human_code) + [1] * len(ai_code)\n",
        "\n",
        "        return code_samples, labels\n",
        "\n",
        "    def train_traditional_model(self, code_samples, labels):\n",
        "        \"\"\"Train the traditional ML model using TF-IDF and Logistic Regression\"\"\"\n",
        "        print(\"Training traditional model...\")\n",
        "\n",
        "        # Enhanced TF-IDF with better parameters for code\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            max_features=2000,\n",
        "            ngram_range=(1, 3),  # Include bigrams and trigrams\n",
        "            token_pattern=r'(?u)\\b\\w+\\b|[^\\w\\s]',  # Include punctuation as tokens\n",
        "            strip_accents='unicode',\n",
        "            lowercase=True\n",
        "        )\n",
        "\n",
        "        X = self.vectorizer.fit_transform(code_samples)\n",
        "        y = np.array(labels)\n",
        "\n",
        "        # Split with stratification\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Use balanced class weights\n",
        "        self.traditional_model = LogisticRegression(\n",
        "            random_state=42,\n",
        "            class_weight='balanced',\n",
        "            max_iter=1000\n",
        "        )\n",
        "        self.traditional_model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate\n",
        "        y_pred = self.traditional_model.predict(X_test)\n",
        "        print(\"\\nTraditional Model Performance:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # Print most important features\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': self.vectorizer.get_feature_names_out(),\n",
        "            'importance': abs(self.traditional_model.coef_[0])\n",
        "        })\n",
        "        print(\"\\nTop 10 most important features:\")\n",
        "        print(feature_importance.nlargest(10, 'importance'))\n",
        "\n",
        "    def setup_transformer_model(self):\n",
        "        \"\"\"Setup the transformer-based model\"\"\"\n",
        "        print(\"Setting up transformer model...\")\n",
        "        model_name = \"microsoft/codebert-base\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.transformer_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "        self.transformer_model = self.transformer_model.to(self.device)\n",
        "        self.transformer_model.eval()  # Set to evaluation mode\n",
        "\n",
        "    def predict(self, code_snippet):\n",
        "        \"\"\"Make predictions using both models\"\"\"\n",
        "        # Preprocess the input code\n",
        "        code_snippet = self.preprocess_code(code_snippet)\n",
        "\n",
        "        # Traditional model prediction\n",
        "        code_vector = self.vectorizer.transform([code_snippet])\n",
        "        trad_prediction = self.traditional_model.predict(code_vector)[0]\n",
        "        trad_probability = self.traditional_model.predict_proba(code_vector)[0]\n",
        "\n",
        "        # Get feature importance for this prediction\n",
        "        feature_scores = pd.DataFrame({\n",
        "            'feature': self.vectorizer.get_feature_names_out(),\n",
        "            'importance': code_vector.toarray()[0] * self.traditional_model.coef_[0]\n",
        "        })\n",
        "        top_features = feature_scores.nlargest(5, 'importance')\n",
        "\n",
        "        # Transformer model prediction (if setup)\n",
        "        if self.transformer_model is not None:\n",
        "            with torch.no_grad():  # Disable gradient calculation for inference\n",
        "                inputs = self.tokenizer(code_snippet, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "                # Move inputs to GPU\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                outputs = self.transformer_model(**inputs)\n",
        "                logits = outputs.logits.cpu()  # Move back to CPU for numpy conversion\n",
        "                trans_prediction = torch.argmax(logits).item()\n",
        "                trans_probability = torch.softmax(logits, dim=1)[0]\n",
        "\n",
        "            return {\n",
        "                \"traditional\": {\n",
        "                    \"prediction\": \"AI-Generated\" if trad_prediction == 1 else \"Human-Written\",\n",
        "                    \"confidence\": f\"{max(trad_probability):.2%}\",\n",
        "                    \"top_features\": top_features.to_dict('records')\n",
        "                },\n",
        "                \"transformer\": {\n",
        "                    \"prediction\": \"AI-Generated\" if trans_prediction == 1 else \"Human-Written\",\n",
        "                    \"confidence\": f\"{max(trans_probability.detach().numpy()):.2%}\"\n",
        "                }\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            \"traditional\": {\n",
        "                \"prediction\": \"AI-Generated\" if trad_prediction == 1 else \"Human-Written\",\n",
        "                \"confidence\": f\"{max(trad_probability):.2%}\",\n",
        "                \"top_features\": top_features.to_dict('records')\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def batch_predict(self, code_snippets, batch_size=32):\n",
        "        \"\"\"Batch prediction for multiple code snippets\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for i in tqdm(range(0, len(code_snippets), batch_size), desc=\"Batch processing\"):\n",
        "            batch = code_snippets[i:i + batch_size]\n",
        "            batch_results = [self.predict(snippet) for snippet in batch]\n",
        "            results.extend(batch_results)\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6useOn_A1IB",
        "outputId": "d6be6cf7-c0b9-4474-c382-dbbaacdc9002"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing code_classifier.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from code_classifier import CodeClassifier\n",
        "import torch\n",
        "from IPython.display import display, HTML\n",
        "import pandas as pd\n",
        "\n",
        "# Print GPU information\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU device:\", torch.cuda.get_device_name(0))\n",
        "    print(\"GPU memory:\", torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")\n",
        "\n",
        "# Initialize and train the classifier\n",
        "classifier = CodeClassifier()\n",
        "\n",
        "try:\n",
        "    # Prepare and train models\n",
        "    code_samples, labels = classifier.prepare_data()\n",
        "    print(f\"\\nTotal samples loaded: {len(code_samples)}\")\n",
        "\n",
        "    # Train traditional model\n",
        "    classifier.train_traditional_model(code_samples, labels)\n",
        "\n",
        "    # Setup transformer model (now GPU-enabled)\n",
        "    classifier.setup_transformer_model()\n",
        "\n",
        "    # Test the model with some code\n",
        "    test_codes = [\n",
        "        \"\"\"\n",
        "        def calculate_sum(a, b):\n",
        "            return a + b\n",
        "        \"\"\",\n",
        "        \"\"\"\n",
        "        import tensorflow as tf\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.Dense(10, activation='softmax')\n",
        "        ])\n",
        "        \"\"\"\n",
        "    ]\n",
        "\n",
        "    # Process test codes\n",
        "    print(\"\\nTesting with sample code snippets:\")\n",
        "    results = classifier.batch_predict(test_codes)\n",
        "\n",
        "    # Display results in a nice format\n",
        "    for i, (code, result) in enumerate(zip(test_codes, results)):\n",
        "        print(f\"\\nTest Code {i+1}:\")\n",
        "        print(\"```python\")\n",
        "        print(code.strip())\n",
        "        print(\"```\")\n",
        "\n",
        "        for model_type, prediction in result.items():\n",
        "            print(f\"\\n{model_type.title()} Model:\")\n",
        "            print(f\"Classification: {prediction['prediction']}\")\n",
        "            print(f\"Confidence: {prediction['confidence']}\")\n",
        "\n",
        "            if 'top_features' in prediction:\n",
        "                print(\"\\nTop contributing features:\")\n",
        "                feature_df = pd.DataFrame(prediction['top_features'])\n",
        "                display(feature_df)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Please make sure your code samples are in the correct directories\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "H09PurOPBBRb",
        "outputId": "a480c795-946e-4c54-9f35-079890cea17d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'numpy' has no attribute 'dtypes'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-39ab71b8fb12>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcode_classifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCodeClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/code_classifier.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mreplace_return_docstrings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 30\u001b[0;31m from .generic import (\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mContextManagers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mExplicitEnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_flax_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Force early import, allowing use of `jax.core` after importing `jax`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# See PEP 484 & https://github.com/jax-ml/jax/issues/7570\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from jax._src.core import (\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mAbstractToken\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mAbstractToken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mAbstractValue\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mAbstractValue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meffects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/dtypes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;31m# StringDType to be used in there.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0m_string_types\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mJAXType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'StringDType'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mxla_extension_version\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m311\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m   \u001b[0m_string_types\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mJAXType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringDType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mTester\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         raise AttributeError(\"module {!r} has no attribute \"\n\u001b[0m\u001b[1;32m    321\u001b[0m                              \"{!r}\".format(__name__, attr))\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'dtypes'"
          ]
        }
      ]
    }
  ]
}